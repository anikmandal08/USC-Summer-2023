# -*- coding: utf-8 -*-
"""Summer23.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hCIk8uBpaTK-Ipgw-dHeW0eurkoYeRWz

Playing around with the dataset
"""

import numpy as np
from scipy import linalg
import pandas as pd
import cv2
import csv
from google.colab.patches import cv2_imshow
from zipfile import ZipFile
from pandas import *
import matplotlib.pyplot as plt
import scipy
from sklearn.metrics import mean_squared_error
import re
from statistics import mode
import plotly.io as pio
pio.renderers.default = "colab"

!pip install pyLDAvis

!pip install BERTopic

!pip install bertopic[visualization]

from google.colab import drive
drive.mount('/content/drive')

file_name = "/content/drive/MyDrive/complaints.csv.zip"

with ZipFile(file_name, 'r') as zip:
  zip.extractall()
  print('Done')

data = pd.read_csv("complaints.csv")

data_cleaned = data.dropna(subset='Consumer complaint narrative')

data['Company public response'].unique()

df = data_cleaned[data_cleaned['Date received']!=data_cleaned['Date sent to company']]
print(len(df))

print(data.columns)

print(len(data))

print(min(data['Date received']),"   ",max(data['Date received']))

"""Consumer complaint narrative analysis"""

test_str = (data.iloc[0]['Consumer complaint narrative'])

res = re.sub(r'[^\w\s]', '', test_str)

print(res)

import datetime as dt
data_cleaned['Date received'] = pd.to_datetime(data_cleaned['Date received'])
data['Date received'] = pd.to_datetime(data['Date received'])

"""Yearwise complaint count"""

years = [2015,2016,2017,2018,2019,2020,2021,2022]

for year in years:
  dataframe1=data_cleaned[data_cleaned['Date received'].dt.year == year ]
  #dataframe1=dataframe1[dataframe1['Product']=='Credit reporting, credit repair services, or other personal consumer reports' ]
  sum = 0
  #print(dataframe1['Product'].value_counts())

  max = 0

  for i in range(len(dataframe1)):

    test_str = (dataframe1.iloc[i]['Consumer complaint narrative'])
    if(str(test_str)!='nan'):
      res = re.sub(r'[^\w\s]', '', test_str)                                #considering only alphanumeric characters and white spaces
#    len_arr.append(len(res));
      sum = sum+ len(res)
      max = np.maximum(max,len(res))
  print(year," No of messages: ",len(dataframe1))
  print("Maximum length of a message: ",max)

  if(len(dataframe1) == 0): print("Average length is: ",sum)
  else : print("Average length is: ",sum/len(dataframe1))

arr = data['State'].value_counts()
print(type(arr))

data_cleaned['Consumer consent provided?'].value_counts()

data['Company'].value_counts()[:5]

print("No of entries in cleaned data: ",len(data_cleaned))
len_arr=[]
sum = 0
for i in range(len(data_cleaned)):
  test_str = (data_cleaned.iloc[i]['Consumer complaint narrative'])
  res = re.sub(r'[^\w\s]', '', test_str)                                #considering only alphanumeric characters and white spaces
  res = re.sub(r'[\bX]','',res)
  len_arr.append(len(res));
  sum = sum+ len(res)
print("Average length is: ",sum/len(data_cleaned))                                            #mean length of complaint text

print("Mode of the length is: ",mode(len_arr))                                                     #mode of length of complaint text

"""**Visual Plots**

Count of consumer complaints of issues of each of the different products
"""

data['Company response to consumer'].unique()

arr = data['Product'].unique()
for prod in arr:
  tmp_data = data[data['Product']==prod]
  print(prod)
  print("\n\n")
  #print(tmp_data['Issue'].value_counts())
  #print("\n\n")

  label = tmp_data['Issue'].unique()

  label = [x for x in label if str(x) != 'nan']
  #print(len(label))

  num = tmp_data['Issue'].value_counts()
  #print(len(num))

  arr=[]
  for i in range(len(num)):
    arr.append(num[label[i]])

  fig = plt.figure(figsize =(15, 10))
  #print(arr)
  #print(label)
  plt.bar((label), (arr),color = 'maroon',width=0.3)
  plt.xticks(rotation=90)
  plt.title(prod)

  plt.xlabel('Issue-->')
  plt.ylabel('Consumer complaint count-->')

  plt.show()

#print(num)
#print(s)

"""Consumer complaints vs Products"""

label = data['Product'].unique()

label = [x for x in label if str(x) != 'nan']
print(len(label))

num = data['Product'].value_counts()
print(len(num))

arr=[]
for i in range(len(num)):
  arr.append(num[label[i]])

#print(num)

"""Finding the most frequent word in the complaint text"""

data_dominant = data_cleaned[data_cleaned['Product']== "Credit reporting, credit repair services, or other personal consumer reports"]

data_ = (data_dominant.sample(10)['Consumer complaint narrative'])
arr=[]
for i in range(10):
  test_str = (data_.iloc[i])
  res = re.sub(r'[^\w\s]', '', test_str)                                #considering only alphanumeric characters and white spaces
  res = re.sub(r'[\bX]','',res)
  arr.append(res)
  print(res)

#data_.to_csv("sample_10.csv")

from collections import defaultdict
temp = defaultdict(int)

# memoizing count
for sub in arr:
    for wrd in sub.split():
        temp[wrd] += 1

# getting max frequency
res = max(temp, key=temp.get)

# printing result
print("Word with maximum frequency : " + str(res))

fig = plt.figure(figsize =(20, 10))
print(arr)
print(label)
plt.bar((label), (arr),color = 'maroon',width=0.5)
plt.xticks(rotation=90)

plt.xlabel('Product-->')
plt.ylabel('Consumer complaint count-->')

plt.show()

"""Consumer complaint vs State"""

label = ['AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'DC', 'FL', 'GA', 'HI','ID', 'IL', 'IN', 'IA','KS', 'KY', 'LA', 'ME', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY','NC','ND', 'OH', 'OK', 'OR', 'PA', 'PR','RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY']

pop =[5074296,733583,7359197,3045637,39029342,5839926,3626205,1018396,671803,22244823,10912876,1440196,1939033,12582032,6833037,3200517,2937150,4512310,4590241,1385340,6164660,6981974,10034113,5717184,2940057,6177957,1122867,1967923,3177772,1395231,9261699,2113344,19677151,10698973,779261,  11756058	,  4019800,  4240137,  12972008,  3221789,  1093734,  5282634,  909824,  7051339,  30029572,  3380800,  647064,  8683619,  7785786,  1775156,  5892539,  581381]

print(pop[32])

blue = ['CA','CO','CT','DE','DC','HI','IL','ME','MD','MA','NH','NJ','NM','NY','OR','RI','VT','VA','WA']
red = ['AL','AK','AR','ID','IN','IA','KS','KY','LA','MS','MO','MT','NE','ND','OK','SC','SD','TN','TX','UT','WV','WY']

for state in label:
  data_state = data[data['State']==state]
  years = [2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022]

  print(state)

  for year in years:
    data_year=data_state[data_state['Date received'].dt.year == year ]
    print(year,len(data_year))

x = [2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022]
y = [11672,16971,21442,23814,26872,31594,32586,34010,54040,56641,80092]
plt.plot(x, y, 'o-r')
plt.xlabel('Year-->')
plt.ylabel('Consumer complaint number-->')
plt.title('Variation of complaint count in the state of California')
plt.grid()

data_filtered = data[data['State'] == 'NY']
data_filtered = data_filtered[data_filtered['Date received'].dt.year==2020]
print(data_filtered['ZIP code'].value_counts()[4:])

"""CA NY





---






90046 --> 27220,3385292              11434 --> 34,530,1650352


---


90025 --> 25440, 5419582              11550 -->31,410,1396402

93722 --> 39490, 2046091             11236 -->49020,2385678

91722 --> 18760, 1016497             11003 -->25530,1507900

Without removing nan values
"""

#label = data['State'].unique()

label = [x for x in label if str(x) != 'nan']
print(len(label))

#label.sort()

num = data['State'].value_counts()
print(len(num))

arr=[]
for i in range(len(label)):
  print(label[i],pop[i])
  arr.append(num[label[i]])

print(num)

fig = plt.figure(figsize =(30, 10))
print(arr)
print(label)
plt.bar((label), (arr),color = 'maroon',width=0.45)
plt.xticks(rotation=90)

plt.xlabel('State-->')
plt.ylabel('Consumer complaint count-->')

plt.show()
#plt.legend()

"""After removing nan values from the column 'Consumer complaint narrative'."""

blue = ['CA','CO','CT','DE','DC','HI','IL','ME','MD','MA','NH','NJ','NM','NY','OR','RI','VT','VA','WA']
red = ['AL','AK','AR','ID','IN','IA','KS','KY','LA','MS','MO','MT','NE','ND','OK','SC','SD','TN','TX','UT','WV','WY']

cnt = 0
for state in blue:
  data_state = data[data['State'] == state]
  for i in range(len(pop)):
    if(label[i] == state):
      cnt = cnt+(len(data_state)/pop[i])

print("Blue State per capita complaint cnt: ",cnt)
cnt = 0

for state in red:
  data_state = data[data['State'] == state]
  for i in range(len(pop)):
    if(label[i] == state):
      cnt = cnt+(len(data_state)/pop[i])

print("Red State per capita complaint cnt: ",cnt)

label = [x for x in label if str(x) != 'nan']
print(len(label))

#label.sort()

num = data_cleaned['State'].value_counts()
print(len(num))

arr=[]
for i in range(len(label)):
  arr.append(num[label[i]]/pop[i])

print(num)

fig = plt.figure(figsize =(30, 10))
print(arr)
print(label)
plt.bar((label), (arr),color = 'maroon',width=0.45)
plt.xticks(rotation=90)

plt.xlabel('State-->')
plt.ylabel('Consumer complaint count-->')

plt.show()

"""Consumer complaint vs consumer consent provided?"""

label = data['Consumer consent provided?'].unique()

label = [x for x in label if str(x) != 'nan']
print(len(label))

num = data['Consumer consent provided?'].value_counts()
print(len(num))

arr=[]
for i in range(len(num)):
  arr.append(num[label[i]])

print(num)

fig = plt.figure(figsize =(6, 4))
print(arr)
print(label)
plt.bar((label), (arr),color = 'maroon',width=0.45)
plt.xticks(rotation=45)

plt.xlabel('Consumer consent-->')
plt.ylabel('Consumer complaint count-->')

plt.show()
#plt.legend()

"""Consumer complaint vs submitted via?"""

label = data['Submitted via'].unique()

label = [x for x in label if str(x) != 'nan']
print(len(label))

num = data['Submitted via'].value_counts()
print(len(num))

arr=[]
for i in range(len(num)):
  arr.append(num[label[i]])

print(num)

fig = plt.figure(figsize =(10, 7))
print(arr)
print(label)
plt.bar((label), (arr),color = 'maroon',width=0.45)
plt.xticks(rotation=45)

plt.xlabel('Submitted via-->')
plt.ylabel('Consumer complaint count-->')

plt.show()

fig, ax = plt.subplots()
ax.pie(arr, labels=label)
plt.show()
#plt.legend()

"""Consumer complaints vs Company response to consumer"""

label = data['Company response to consumer'].unique()

label = [x for x in label if str(x) != 'nan']
print(len(label))

num = data['Company response to consumer'].value_counts()
print(len(num))

arr=[]
for i in range(len(num)):
  arr.append(num[label[i]])

print(num)

fig = plt.figure(figsize =(10, 7))
print(arr)
print(label)
plt.bar((label), (arr),color = 'maroon',width=0.45)
plt.xticks(rotation=90)

plt.xlabel('Company response to consumer-->')
plt.ylabel('Consumer complaint count-->')

plt.show()

"""Nan count in different columns"""

for col in data.columns:

  nan_count = data[col].isna().sum()
  print(col,":",(nan_count/len(data)) * 100,"%")

years = [2015,2016,2017,2018,2019,2020,2021,2022]
cnt=0
data_2015 = data[(data['Date received'].dt.year >= 2015)]
print(len(data_2015))
for year in years:
  data_year = data_2015[data_2015['Date received'].dt.year == year]
  nan_count = data_year['Consumer complaint narrative'].isna().sum()
  print(year," : ",nan_count/len(data_year) * 100,"%")

for col in data.columns:

  nan_count = data_2015[col].isna().sum()
  print(col,":",(nan_count/len(data_2015)) * 100,"%")
prod_arr = data_2015['Product'].unique()
print("\n\n")
for prod in prod_arr:
  data_prod = data_2015[data_2015['Product'] == prod]
  print(prod,":",(len(data_prod)/len(data_2015)) * 100,"%")

state_arr = data_2015['State'].unique().tolist()
state_arr = [x for x in state_arr if str(x) != 'nan']
#state_arr.remove(nan)

state_arr.sort()
print("\n\n")

for state in state_arr:
  data_state = data_2015[data_2015['State'] == state]
  print(state,":",(len(data_state)/len(data_2015)) * 100,"%")

years = [2015,2016,2017,2018,2019,2020,2021,2022]
cnt=0
data_2015 = data[(data['Date received'].dt.year <= 2014)]
print(len(data_2015))

for col in data.columns:

  nan_count = data_2015[col].isna().sum()
  print(col,":",(nan_count/len(data_2015)) * 100,"%")
prod_arr = data_2015['Product'].unique()
print("\n\n")
for prod in prod_arr:
  data_prod = data_2015[data_2015['Product'] == prod]
  print(prod,":",(len(data_prod)/len(data_2015)) * 100,"%")

state_arr = data_2015['State'].unique().tolist()
state_arr = [x for x in state_arr if str(x) != 'nan']
#state_arr.remove(nan)

state_arr.sort()
for state in state_arr:
  data_state = data_2015[data_2015['State'] == state]
  print(state,":",(len(data_state)/len(data_2015)) * 100,"%")

prod_arr = data['Product'].unique()

for prod in prod_arr:
  data_prod = data[data['Product'] == prod]
  nan_count = data_prod['Consumer complaint narrative'].isna().sum()
  print(prod,":",(nan_count/len(data_prod)) * 100,"%")

years = [2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022]

for year in years:
    data_year=data[data['Date received'].dt.year == year ]
    data_year_cleaned=data_cleaned[data_cleaned['Date received'].dt.year == year ]
    print(year,len(data_year),len(data_year_cleaned))

state_arr = data_2015['State'].unique()

for state in state_arr:
  data_state = data_2015[data_2015['State'] == state]
  nan_count = data_state['Consumer complaint narrative'].isna().sum()
  print(state,":",(nan_count/len(data_state)) * 100,"%")

"""Performing TF-IDF"""

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.tokenize import word_tokenize

data_2018 = data_cleaned[data_cleaned['Date received'].dt.year == 2018]

data_2019 = data_cleaned[data_cleaned['Date received'].dt.year == 2019]

data_2020 = data_cleaned[data_cleaned['Date received'].dt.year == 2020]

data_2021 = data_cleaned[data_cleaned['Date received'].dt.year == 2021]
data_2022 = data_cleaned[data_cleaned['Date received'].dt.year == 2022]

#data = pd.concat([data, data_tmp], ignore_index=True)
data_2018_19 = pd.concat([data_2018,data_2019],ignore_index = True)
data_2020_21 = pd.concat([data_2020,data_2021],ignore_index = True)

import bertopic

import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer
ps = PorterStemmer()
wordnet=WordNetLemmatizer()
nltk.download('wordnet')
corpus = []
corpus_test = []
all_corpus = []

data_2019 = data_2019.sample(100)
data_2020 = data_2020.sample(100)

for i in range(len(data_2019)):
    review = data_2019.iloc[i]['Consumer complaint narrative']
    print(i)
    print(review)
    print("\n\n")

    #review = re.sub('[^a-zA-Z]', ' ', data_2020.iloc[i]['Consumer complaint narrative'])
    #review = re.sub(r'[\bX]',' ',review)
    #print(review)
    review = review.lower()
    review = review.split()
    review = [wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]
    review = ' '.join(review)
    #print(review)
    all_corpus.append(review)
#print((corpus[0]))
# Creating the TF-IDF model
from sklearn.feature_extraction.text import TfidfVectorizer
cv = TfidfVectorizer()
vectors1 = cv.fit_transform(all_corpus).toarray()
vocab = (cv.get_feature_names_out())   #vocabulary

#vocab = (cv.get_params())

#print(vocab)

#vectorizer = TfidfVectorizer()

vectors = []
vectors_test = []

for i in range(len(vectors1)):
  if(i<50):vectors.append(vectors1[i])
  else: vectors_test.append(vectors1[i])

def F(a,b):
  cos_sim = dot(a, b)/(norm(a)*norm(b))
  return cos_sim

from numpy import dot
from numpy.linalg import norm
def F(a,b):
  cos_sim = dot(a, b)/(norm(a)*norm(b))
  return cos_sim

for i in range(50):
  for j in range(50):
    print(i,"  ",j," --> ",F(vectors1[i],vectors1[j]))

def FindCommonWords_ForLoop(str1, str2):
    # Convert str1 and str2 into lists of words using str.split() function
    words_str1 = str1.split()
    words_str2 = str2.split()
    # Initialize an empty list that will hold the common words
    common = []
    for word in words_str1:
        # Loop through the words in words_str1
        if word in words_str2 :
            # Append word to common if the word is in words_str2 as well.
            # Only append if the word is not already in the common list.
            common.append(word)
    return common
str1 = data_2019.iloc[25]['Consumer complaint narrative'].lower()
str2 = data_2019.iloc[31]['Consumer complaint narrative'].lower()
for i in range(50):
  str2 = data_2019.iloc[i]['Consumer complaint narrative'].lower()

  print(i,len(FindCommonWords_ForLoop(str1,str2)))

#take 1 random sample of complaint narrative
#data_2018 = data_2018.sample(4000)
#data_2019 = data_2019.sample(4000)
#data_2020 = data_2020.sample(4000)
#data_2021 = data_2021.sample(4000)
#data_2022 = data_2022.sample(4000)

data = pd.DataFrame()

prod_arr = data_2019['Product'].unique()
for prod in prod_arr:
  data_tmp = data_2019[data_2019['Product']== prod].sample(50)
  data = pd.concat([data, data_tmp], ignore_index=True)

  #data = data.append(data_tmp)

#data = data_2019.sample(1000)

import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer
ps = PorterStemmer()
wordnet=WordNetLemmatizer()
nltk.download('wordnet')
all_corpus = []
for i in range(len(data)):
    review = data.iloc[i]['Consumer complaint narrative']

    #review = re.sub('[^a-zA-Z]', ' ', data_2020.iloc[i]['Consumer complaint narrative'])
    review = re.sub(r'[\bX]',' ',review)
    #print(review)
    review = review.lower()
    review = review.split()
    review = [wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]
    review = ' '.join(review)
    #print(review)
    all_corpus.append(review)
print(data['Product'].value_counts())

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
#from bertopic.dimensionality import BaseDimensionalityReduction
import math

import gensim
from gensim.utils import simple_preprocess
import gensim.corpora as corpora
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
stop_words = stopwords.words('english')
import pyLDAvis
import pyLDAvis.gensim_models
import pickle
#def sent_to_words(sentences):
def sent_to_words(sentences):
    for sentence in sentences:
        # deacc=True removes punctuations
        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))
def remove_stopwords(texts):
    return [[word for word in simple_preprocess(str(doc))
             if word not in stop_words] for doc in texts]
from bertopic import BERTopic
sil_max = -1
prod_arr = data['Product'].unique()
cv = TfidfVectorizer()
for num in range(5,16,5):
  Kmean = KMeans(n_clusters=num,n_init=12,random_state=10)
  vectors = cv.fit_transform(all_corpus).toarray()
  Kmean.fit(vectors)
  vectors = pd.DataFrame(vectors)
  clusters = Kmean.predict(vectors)
  #print(clusters)

  vectors["Cluster"] = clusters
  arr = []


  for i in range(len(all_corpus)):
    arr.append(data.iloc[i]['Product'])



  vectors["Product"] = arr

  arr = []
  for i in range(len(all_corpus)):
    arr.append(all_corpus[i])



  vectors["Narrative"] = arr



  #sil = silhouette_score(vectors, clusters)
  #print("Silhouette Score: n = ",num, "is : ",sil)
  #if(sil > sil_max):
  #  sil_max=sil
  #  opt = num
  print("\n")
  print("Total number of clusters: ",num)
  tot_ent = 0
  for i in range(num):
    df = vectors[vectors['Cluster'] == i]
    lst = []
    for j in range(len(df)):
      lst.append(df.iloc[j]['Narrative'])
    stopwords = set(STOPWORDS)
    text = " ".join(review for review in df.Narrative)

    wordcloud = WordCloud(stopwords=stopwords, background_color="white").generate(text)


    # Display the generated image:
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")
    plt.show()



    # number of topics
    tmp = (df['Product'].value_counts())





    print(tmp)

    ent = 0
    print("Cluster no: ",i,sep = "  ")
    for j in range(len(tmp)):
      ent = ent + (((tmp[j])/len(df)) * (math.log((tmp[j])/len(df),9)))
    print("Entropy = ",-1*ent)
    print("Size of cluster = ",len(df))
    print("\n\n")

    model = BERTopic(language="english", calculate_probabilities=True, verbose=True,min_topic_size = 2)

    print(len(lst))

    if(len(lst)>5):
        topics, probabilities = model.fit_transform(lst)




        print(model.get_topic_freq())
        #model.visualize_topics()
        if(len(model.get_topic_freq()) > 1):
          fig = model.visualize_barchart()
          fig.write_html("result_2019_"+str(num)+"_"+str(i)+".html")

    else: print("Total number of documents is lesser than equal to 5")

#convert to list
      #docs = df.text.to_list()

    tot_ent = tot_ent + -1*ent*(len(df)/len(vectors))

  print("For number of clusters = ",num," total entropy is ",tot_ent)

#print("optimum number of clusters are: ",opt)

data = pd.DataFrame()

prod_arr = data_2020['Product'].unique()
for prod in prod_arr:
  data_tmp = data_2020[data_2020['Product']== prod].sample(50)
  data = pd.concat([data, data_tmp], ignore_index=True)

#data_2020 monthwise

for month in range(1,13):
  data_month = data_2020[data_2020['Date received'].dt.month == month]
  print(month,len(data_month))

import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer
ps = PorterStemmer()
wordnet=WordNetLemmatizer()
nltk.download('wordnet')
all_corpus = []
for i in range(len(data)):
    review = data.iloc[i]['Consumer complaint narrative']

    #review = re.sub('[^a-zA-Z]', ' ', data_2020.iloc[i]['Consumer complaint narrative'])
    review = re.sub(r'[\bX]',' ',review)
    #print(review)
    review = review.lower()
    review = review.split()
    review = [wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]
    if('covid-19' in review or 'covid' in  review or 'pandemic' in review or 'coronavirus' in review):
      print(data.iloc[i]['Product'])
    review = ' '.join(review)
    #print(review)
    all_corpus.append(review)
print(data['Product'].value_counts())

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
#from bertopic.dimensionality import BaseDimensionalityReduction
import math
#from bertopic import BERTopic
sil_max = -1
prod_arr = data['Product'].unique()
cv = TfidfVectorizer()
for num in range(5,16,5):
  Kmean = KMeans(n_clusters=num,n_init=12,random_state=10)
  vectors = cv.fit_transform(all_corpus).toarray()
  Kmean.fit(vectors)
  vectors = pd.DataFrame(vectors)
  clusters = Kmean.predict(vectors)
  #print(clusters)

  vectors["Cluster"] = clusters
  arr = []


  for i in range(len(all_corpus)):
    arr.append(data.iloc[i]['Product'])



  vectors["Product"] = arr

  arr = []
  for i in range(len(all_corpus)):
    arr.append(all_corpus[i])



  vectors["Narrative"] = arr



  #sil = silhouette_score(vectors, clusters)
  #print("Silhouette Score: n = ",num, "is : ",sil)
  #if(sil > sil_max):
  #  sil_max=sil
  #  opt = num
  print("\n")
  print("Total number of clusters: ",num)
  tot_ent = 0
  for i in range(num):
    df = vectors[vectors['Cluster'] == i]
    lst = []
    for j in range(len(df)):
      lst.append(df.iloc[j]['Narrative'])
    stopwords = set(STOPWORDS)
    text = " ".join(review for review in df.Narrative)

    wordcloud = WordCloud(stopwords=stopwords, background_color="white").generate(text)


    # Display the generated image:
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")
    plt.show()



    tmp = (df['Product'].value_counts())
    print(tmp)

    ent = 0
    print("Cluster no: ",i,sep = "  ")
    for j in range(len(tmp)):
      ent = ent + (((tmp[j])/len(df)) * (math.log((tmp[j])/len(df),9)))
    print("Entropy = ",-1*ent)
    print("Size of cluster = ",len(df))
    print("\n\n")
    model = BERTopic(language="english", calculate_probabilities=True, verbose=True,min_topic_size = 2)

    print(len(lst))
    if(len(lst)>5):
        topics, probabilities = model.fit_transform(lst)




        print(model.get_topic_freq())
        #model.visualize_topics()
        if(len(model.get_topic_freq()) > 1):
          fig = model.visualize_barchart()
          fig.write_html("result_2020_"+str(num)+"_"+str(i)+".html")

    else: print("Number of documents in the cluster in lesser than equal to 5")
    tot_ent = tot_ent + -1*ent*(len(df)/len(vectors))

  print("For number of clusters = ",num," total entropy is ",tot_ent)

#print("optimum number of clusters are: ",opt)

data = pd.DataFrame()

prod_arr = data_2018['Product'].unique()
for prod in prod_arr:
  data_tmp = data_2018[data_2018['Product']== prod].sample(50)
  data = pd.concat([data, data_tmp], ignore_index=True)

import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer
ps = PorterStemmer()
wordnet=WordNetLemmatizer()
nltk.download('wordnet')
all_corpus = []
for i in range(len(data)):
    review = data.iloc[i]['Consumer complaint narrative']

    #review = re.sub('[^a-zA-Z]', ' ', data_2020.iloc[i]['Consumer complaint narrative'])
    review = re.sub(r'[\bX]',' ',review)
    #print(review)
    review = review.lower()
    review = review.split()
    review = [wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]
    review = ' '.join(review)
    #print(review)
    all_corpus.append(review)
print(data['Product'].value_counts())

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
#from bertopic.dimensionality import BaseDimensionalityReduction
import math
from bertopic import BERTopic
sil_max = -1
prod_arr = data['Product'].unique()
cv = TfidfVectorizer()
for num in range(5,16,5):
  Kmean = KMeans(n_clusters=num,n_init=12,random_state=10)
  vectors = cv.fit_transform(all_corpus).toarray()
  Kmean.fit(vectors)
  vectors = pd.DataFrame(vectors)
  clusters = Kmean.predict(vectors)
  #print(clusters)

  vectors["Cluster"] = clusters
  arr = []


  for i in range(len(all_corpus)):
    arr.append(data.iloc[i]['Product'])



  vectors["Product"] = arr

  arr = []
  for i in range(len(all_corpus)):
    arr.append(all_corpus[i])



  vectors["Narrative"] = arr



  #sil = silhouette_score(vectors, clusters)
  #print("Silhouette Score: n = ",num, "is : ",sil)
  #if(sil > sil_max):
  #  sil_max=sil
  #  opt = num
  print("\n")
  print("Total number of clusters: ",num)
  tot_ent = 0
  for i in range(num):
    df = vectors[vectors['Cluster'] == i]
    lst = []
    for j in range(len(df)):
      lst.append(df.iloc[j]['Narrative'])
    stopwords = set(STOPWORDS)
    text = " ".join(review for review in df.Narrative)

    wordcloud = WordCloud(stopwords=stopwords, background_color="white").generate(text)


    # Display the generated image:
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")
    plt.show()



    tmp = (df['Product'].value_counts())
    print(tmp)

    ent = 0
    print("Cluster no: ",i,sep = "  ")
    for j in range(len(tmp)):
      ent = ent + (((tmp[j])/len(df)) * (math.log((tmp[j])/len(df),9)))
    print("Entropy = ",-1*ent)
    print("Size of cluster = ",len(df))
    print("\n\n")
    model = BERTopic(language="english", calculate_probabilities=True, verbose=True,min_topic_size = 2)

    print(len(lst))
    if(len(lst)>5):
        topics, probabilities = model.fit_transform(lst)




        print(model.get_topic_freq())
        #model.visualize_topics()
        if(len(model.get_topic_freq()) > 1):
          fig = model.visualize_barchart()
          fig.write_html("result_2018_"+str(num)+"_"+str(i)+".html")

    else: print("Number of documents in the cluster is less than equal to 5")
    tot_ent = tot_ent + -1*ent*(len(df)/len(vectors))

  print("For number of clusters = ",num," total entropy is ",tot_ent)

#print("optimum number of clusters are: ",opt)

data = pd.DataFrame()

prod_arr = data_2021['Product'].unique()
for prod in prod_arr:
  data_tmp = data_2021[data_2021['Product']== prod].sample(50)
  data = pd.concat([data, data_tmp], ignore_index=True)

import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer
ps = PorterStemmer()
wordnet=WordNetLemmatizer()
nltk.download('wordnet')
all_corpus = []
for i in range(len(data)):
    review = data.iloc[i]['Consumer complaint narrative']

    #review = re.sub('[^a-zA-Z]', ' ', data_2020.iloc[i]['Consumer complaint narrative'])
    review = re.sub(r'[\bX]',' ',review)
    #print(review)
    review = review.lower()
    review = review.split()
    review = [wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]
    if('covid-19' in review or 'covid' in  review or 'pandemic' in review or 'coronavirus' in review):
      print(data.iloc[i]['Product'])
    review = ' '.join(review)
    #print(review)
    all_corpus.append(review)
print(data['Product'].value_counts())

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
#from bertopic.dimensionality import BaseDimensionalityReduction
import math
from bertopic import BERTopic
sil_max = -1
prod_arr = data['Product'].unique()
cv = TfidfVectorizer()
for num in range(5,16,5):
  Kmean = KMeans(n_clusters=num,n_init=12,random_state=10)
  vectors = cv.fit_transform(all_corpus).toarray()
  Kmean.fit(vectors)
  vectors = pd.DataFrame(vectors)
  clusters = Kmean.predict(vectors)
  #print(clusters)

  vectors["Cluster"] = clusters
  arr = []


  for i in range(len(all_corpus)):
    arr.append(data.iloc[i]['Product'])



  vectors["Product"] = arr

  arr = []
  for i in range(len(all_corpus)):
    arr.append(all_corpus[i])



  vectors["Narrative"] = arr



  #sil = silhouette_score(vectors, clusters)
  #print("Silhouette Score: n = ",num, "is : ",sil)
  #if(sil > sil_max):
  #  sil_max=sil
  #  opt = num
  print("\n")
  print("Total number of clusters: ",num)
  tot_ent = 0
  for i in range(num):
    df = vectors[vectors['Cluster'] == i]
    lst = []
    for j in range(len(df)):
      lst.append(df.iloc[j]['Narrative'])
    stopwords = set(STOPWORDS)
    text = " ".join(review for review in df.Narrative)

    wordcloud = WordCloud(stopwords=stopwords, background_color="white").generate(text)


    # Display the generated image:
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")
    plt.show()



    tmp = (df['Product'].value_counts())
    print(tmp)

    ent = 0
    print("Cluster no: ",i,sep = "  ")
    for j in range(len(tmp)):
      ent = ent + (((tmp[j])/len(df)) * (math.log((tmp[j])/len(df),9)))
    print("Entropy = ",-1*ent)
    print("Size of cluster = ",len(df))
    print("\n\n")
    model = BERTopic(language="english", calculate_probabilities=True, verbose=True,min_topic_size = 2)

    print(len(lst))
    if(len(lst)>5):
        topics, probabilities = model.fit_transform(lst)
        print(model.get_topic(0))




        print(model.get_topic_freq())
        #model.visualize_topics()
        fig = model.visualize_barchart()
        fig.write_html("result_2021_"+str(num)+"_"+str(i)+".html")

    else: print("Number of documents in the cluster is lesser than equal to 5")
    tot_ent = tot_ent + -1*ent*(len(df)/len(vectors))

  print("For number of clusters = ",num," total entropy is ",tot_ent)

#print("optimum number of clusters are: ",opt)

data = pd.DataFrame()

prod_arr = data_2022['Product'].unique()
for prod in prod_arr:
  data_tmp = data_2022[data_2022['Product']== prod].sample(50)
  data = pd.concat([data, data_tmp], ignore_index=True)

import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer
ps = PorterStemmer()
wordnet=WordNetLemmatizer()
nltk.download('wordnet')
all_corpus = []
for i in range(len(data)):
    review = data.iloc[i]['Consumer complaint narrative']

    #review = re.sub('[^a-zA-Z]', ' ', data_2020.iloc[i]['Consumer complaint narrative'])
    review = re.sub(r'[\bX]',' ',review)
    #print(review)
    review = review.lower()
    review = review.split()
    review = [wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]
    review = ' '.join(review)
    if('covid-19' in review or 'covid' in  review or 'pandemic' in review or 'coronavirus' in review):
      print(data.iloc[i]['Product'])
    #print(review)
    all_corpus.append(review)
print(data['Product'].value_counts())

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
#from bertopic.dimensionality import BaseDimensionalityReduction
import math
from bertopic import BERTopic
sil_max = -1
prod_arr = data['Product'].unique()
cv = TfidfVectorizer()
for num in range(5,16,5):
  Kmean = KMeans(n_clusters=num,n_init=12,random_state=10)
  vectors = cv.fit_transform(all_corpus).toarray()
  Kmean.fit(vectors)
  vectors = pd.DataFrame(vectors)
  clusters = Kmean.predict(vectors)
  #print(clusters)

  vectors["Cluster"] = clusters
  arr = []


  for i in range(len(all_corpus)):
    arr.append(data.iloc[i]['Product'])



  vectors["Product"] = arr

  arr = []
  for i in range(len(all_corpus)):
    arr.append(all_corpus[i])



  vectors["Narrative"] = arr



  #sil = silhouette_score(vectors, clusters)
  #print("Silhouette Score: n = ",num, "is : ",sil)
  #if(sil > sil_max):
  #  sil_max=sil
  #  opt = num
  print("\n")
  print("Total number of clusters: ",num)
  tot_ent = 0
  for i in range(num):
    df = vectors[vectors['Cluster'] == i]
    lst = []
    for j in range(len(df)):
      lst.append(df.iloc[j]['Narrative'])
    stopwords = set(STOPWORDS)
    text = " ".join(review for review in df.Narrative)

    wordcloud = WordCloud(stopwords=stopwords, background_color="white").generate(text)


    # Display the generated image:
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")
    plt.show()



    tmp = (df['Product'].value_counts())
    print(tmp)

    ent = 0
    print("Cluster no: ",i,sep = "  ")
    for j in range(len(tmp)):
      ent = ent + (((tmp[j])/len(df)) * (math.log2((tmp[j])/len(df))))
    print("Entropy = ",-1*ent)
    print("Size of cluster = ",len(df))
    print("\n\n")
    model = BERTopic(language="english", calculate_probabilities=True, verbose=True,min_topic_size = 2)

    print(len(lst))
    if(len(lst)>5):
        topics, probabilities = model.fit_transform(lst)

        print(model.get_topic(0))




        print(model.get_topic_freq())
        #model.visualize_topics()
        fig = model.visualize_barchart()
        fig.write_html("result_2022_"+str(num)+"_"+str(i)+".html")

    else: print("Number of documents in the cluster is lesser than equal to 5")
    tot_ent = tot_ent + -1*ent*(len(df)/len(vectors))

  print("For number of clusters = ",num," total entropy is ",tot_ent)

#print("optimum number of clusters are: ",opt)

data = pd.DataFrame()

prod_arr = data_2018_19['Product'].unique()
for prod in prod_arr:
  data_tmp = data_2018_19[data_2018_19['Product']== prod].sample(200)
  data = pd.concat([data, data_tmp], ignore_index=True)

  #data = data.append(data_tmp)

import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer
ps = PorterStemmer()
wordnet=WordNetLemmatizer()
nltk.download('wordnet')
all_corpus = []
for i in range(len(data)):
    review = data.iloc[i]['Consumer complaint narrative']

    #review = re.sub('[^a-zA-Z]', ' ', data_2020.iloc[i]['Consumer complaint narrative'])
    review = re.sub(r'[\bX]',' ',review)
    #print(review)
    review = review.lower()
    #review = review.split()
    #review = [wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]
    #review = ' '.join(review)
    #print(review)
    all_corpus.append(review)
print(data['Product'].value_counts())

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
#from bertopic.dimensionality import BaseDimensionalityReduction
import math
from bertopic import BERTopic
sil_max = -1
prod_arr = data['Product'].unique()
cv = TfidfVectorizer()
for num in range(9,10):
  Kmean = KMeans(n_clusters=num,n_init=12,random_state=10)
  vectors = cv.fit_transform(all_corpus).toarray()
  Kmean.fit(vectors)
  vectors = pd.DataFrame(vectors)
  clusters = Kmean.predict(vectors)
  #print(clusters)

  vectors["Cluster"] = clusters
  arr = []


  for i in range(len(all_corpus)):
    arr.append(data.iloc[i]['Product'])



  vectors["Product"] = arr

  arr = []
  for i in range(len(all_corpus)):
    arr.append(all_corpus[i])



  vectors["Narrative"] = arr



  #sil = silhouette_score(vectors, clusters)
  #print("Silhouette Score: n = ",num, "is : ",sil)
  #if(sil > sil_max):
  #  sil_max=sil
  #  opt = num
  print("\n")
  print("Total number of clusters: ",num)
  tot_ent = 0
  for i in range(num):
    df = vectors[vectors['Cluster'] == i]
    lst = []
    for j in range(len(df)):
      lst.append(df.iloc[j]['Narrative'])
    stopwords = set(STOPWORDS)
    text = " ".join(review for review in df.Narrative)

    wordcloud = WordCloud(stopwords=stopwords, background_color="white").generate(text)


    # Display the generated image:
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")
    plt.show()



    tmp = (df['Product'].value_counts())
    print(tmp)

    ent = 0
    print("Cluster no: ",i,sep = "  ")
    for j in range(len(tmp)):
      ent = ent + (((tmp[j])/len(df)) * (math.log((tmp[j])/len(df),9)))
    print("Entropy = ",-1*ent)
    print("Size of cluster = ",len(df))
    print("\n\n")
    model = BERTopic(language="english", calculate_probabilities=True, verbose=True,min_topic_size = 10)

    print(len(lst))
    if(len(lst)>5):
        topics, probabilities = model.fit_transform(lst)

        print(model.get_topic(0))




        print(model.get_topic_freq())
        #model.visualize_topics()
        fig = model.visualize_barchart()
        fig.write_html("result_2018_19_"+str(num)+"_"+str(i)+".html")

    else: print("Number of documents in the cluster is lesser than equal to 5")
    tot_ent = tot_ent + -1*ent*(len(df)/len(vectors))

  print("For number of clusters = ",num," total entropy is ",tot_ent)

#print("optimum number of clusters are: ",opt)

data = pd.DataFrame()

prod_arr = data_2020_21['Product'].unique()
for prod in prod_arr:
  data_tmp = data_2020_21[data_2020_21['Product']== prod].sample(200)
  data = pd.concat([data, data_tmp], ignore_index=True)

import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer
ps = PorterStemmer()
wordnet=WordNetLemmatizer()
nltk.download('wordnet')
all_corpus = []
for i in range(len(data)):
    review = data.iloc[i]['Consumer complaint narrative']

    #review = re.sub('[^a-zA-Z]', ' ', data_2020.iloc[i]['Consumer complaint narrative'])
    review = re.sub(r'[\bX]',' ',review)
    #print(review)
    review = review.lower()
    review = review.split()
    review = [wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]
    review = ' '.join(review)

    #print(review)
    all_corpus.append(review)
print(data['Product'].value_counts())

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
#from bertopic.dimensionality import BaseDimensionalityReduction
import math
from bertopic import BERTopic
sil_max = -1
prod_arr = data['Product'].unique()
cv = TfidfVectorizer()
for num in range(9,10):
  Kmean = KMeans(n_clusters=num,n_init=12,random_state=10)
  vectors = cv.fit_transform(all_corpus).toarray()
  Kmean.fit(vectors)
  vectors = pd.DataFrame(vectors)
  clusters = Kmean.predict(vectors)
  #print(clusters)

  vectors["Cluster"] = clusters
  arr = []


  for i in range(len(all_corpus)):
    arr.append(data.iloc[i]['Product'])



  vectors["Product"] = arr

  arr = []
  for i in range(len(all_corpus)):
    arr.append(all_corpus[i])



  vectors["Narrative"] = arr



  #sil = silhouette_score(vectors, clusters)
  #print("Silhouette Score: n = ",num, "is : ",sil)
  #if(sil > sil_max):
  #  sil_max=sil
  #  opt = num
  print("\n")
  print("Total number of clusters: ",num)
  tot_ent = 0
  for i in range(num):
    df = vectors[vectors['Cluster'] == i]
    lst = []
    for j in range(len(df)):
      lst.append(df.iloc[j]['Narrative'])
    stopwords = set(STOPWORDS)
    text = " ".join(review for review in df.Narrative)

    wordcloud = WordCloud(stopwords=stopwords, background_color="white").generate(text)


    # Display the generated image:
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")
    plt.show()



    tmp = (df['Product'].value_counts())
    print(tmp)

    ent = 0
    print("Cluster no: ",i,sep = "  ")
    for j in range(len(tmp)):
      ent = ent + (((tmp[j])/len(df)) * (math.log((tmp[j])/len(df),9)))
    print("Entropy = ",-1*ent)
    print("Size of cluster = ",len(df))
    print("\n\n")
    model = BERTopic(language="english", calculate_probabilities=True, verbose=True,min_topic_size = 10)

    print(len(lst))
    if(len(lst)>5):
        topics, probabilities = model.fit_transform(lst)

        print(model.get_topic(0))




        print(model.get_topic_freq())
        #model.visualize_topics()
        fig = model.visualize_barchart()
        fig.write_html("result_2020_21_"+str(num)+"_"+str(i)+".html")

    else: print("Number of documents in the cluster is lesser than equal to 5")
    tot_ent = tot_ent + -1*ent*(len(df)/len(vectors))

  print("For number of clusters = ",num," total entropy is ",tot_ent)

#print("optimum number of clusters are: ",opt)

df = data[data['Product'] == 'Credit reporting, credit repair services, or other personal consumer reports']
df = df.sample(50)

from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer
ps = PorterStemmer()
wordnet=WordNetLemmatizer()
nltk.download('wordnet')
lst = []
for j in range(len(df)):
    review = df.iloc[j]['Consumer complaint narrative']
    #print(review)

    #review = re.sub('[^a-zA-Z]', ' ', data_2020.iloc[i]['Consumer complaint narrative'])
    review = re.sub(r'[\bX]',' ',review)
    #print(review)
    review = review.lower()
    #review = review.split()
    #review = [wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]
    #review = ' '.join(review)

    #print(review)
    lst.append(review)
stopwords = set(STOPWORDS)
#print(len(lst))
text = " ".join(review for review in lst)

#print(text)

wordcloud = WordCloud(stopwords=stopwords, background_color="white").generate(text)


# Display the generated image:
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

data_2020_21 = data_2020_21.sample(20000)
import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer
ps = PorterStemmer()
wordnet=WordNetLemmatizer()
nltk.download('wordnet')
all_corpus = []
covid_data = data_2020_21.copy()
covid_data = covid_data[0:0]
for i in range(len(data_2020_21)):
    review = data_2020_21.iloc[i]['Consumer complaint narrative']
    #print(i)

    #review = re.sub('[^a-zA-Z]', ' ', data_2020.iloc[i]['Consumer complaint narrative'])
    #review = re.sub(r'[\bX]',' ',review)
    #print(review)
    review = review.lower()
    review = review.split()
    #review = [wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]
    #review = ' '.join(review)
    if('covid-19' in review or 'covid' in  review or 'pandemic' in review or 'coronavirus' in review):
      covid_data = pd.concat([covid_data,pd.DataFrame([data_2020_21.iloc[i]])],ignore_index=True)
      #df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)
      #covid_data = covid_data.append(data_2020_21.iloc[i])
      #print(data_2020_21.iloc[i])
    #print(review)
    #all_corpus.append(review)

covid_prod = ((covid_data['Product'].value_counts()))
for prod in prod_arr:
  data_prod = data_2020_21[data_2020_21['Product'] == prod]
  print(prod, " ---->>  ", (covid_prod[prod]/len(data_prod)) * 100,"%")

print(covid_data.iloc[1])

data_2020_21 = data_2020_21.sample(10000)
import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer
ps = PorterStemmer()
wordnet=WordNetLemmatizer()
nltk.download('wordnet')
all_corpus = []
for i in range(len(data_2020_21)):
    review = data_2020_21.iloc[i]['Consumer complaint narrative']


    #review = re.sub('[^a-zA-Z]', ' ', data_2020.iloc[i]['Consumer complaint narrative'])
    review = re.sub(r'[\bX]',' ',review)
    #print(review)
    review = review.lower()
    #review = review.split()
    #review = [wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]
    #review = ' '.join(review)

    #print(review)
    all_corpus.append(review)
print(data_2020_21['Product'].value_counts())

from bertopic import BERTopic
model = BERTopic(language="english", calculate_probabilities=True, verbose=True,min_topic_size = 30)

topics, probabilities = model.fit_transform(all_corpus)

print(model.get_topic(0))




print(model.get_topic_freq())
#model.visualize_topics()
fig = model.visualize_barchart()
fig.write_html("result_2020_21_"+".html")

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install bertopic



"""Word cloud for each cluster"""

text = data_2019.iloc[0]['Consumer complaint narrative']

wordcloud = WordCloud().generate(text)

# Display the generated image:
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

print(len(prod_arr))

clusters = Kmean.predict(vectors)
#Add the cluster vector to our DataFrame, X
#vectors["Cluster"] = clusters

from sklearn.metrics import silhouette_score
print(f'Silhouette Score(n=63): {silhouette_score(vectors, clusters)}')

"""PCA"""

from sklearn.decomposition import PCA #Principal Component Analysis
plotX = pd.DataFrame(np.array(vectors.sample(1000)))

#Rename plotX's columns since it was briefly converted to an np.array above
plotX.columns = vectors.columns

pca_1d = PCA(n_components=1)

#PCA with two principal components
pca_2d = PCA(n_components=2)

#PCA with three principal components
pca_3d = PCA(n_components=3)

PCs_1d = pd.DataFrame(pca_1d.fit_transform(plotX.drop(["Cluster"], axis=1)))

#This DataFrame contains the two principal components that will be used
#for the 2-D visualization mentioned above
PCs_2d = pd.DataFrame(pca_2d.fit_transform(plotX.drop(["Cluster"], axis=1)))

#And this DataFrame contains three principal components that will aid us
#in visualizing our clusters in 3-D
PCs_3d = pd.DataFrame(pca_3d.fit_transform(plotX.drop(["Cluster"], axis=1)))

PCs_1d.columns = ["PC1_1d"]

#"PC1_2d" means: 'The first principal component of the components created for 2-D visualization, by PCA.'
#And "PC2_2d" means: 'The second principal component of the components created for 2-D visualization, by PCA.'
PCs_2d.columns = ["PC1_2d", "PC2_2d"]

PCs_3d.columns = ["PC1_3d", "PC2_3d", "PC3_3d"]

plotX = pd.concat([plotX,PCs_1d,PCs_2d,PCs_3d], axis=1, join='inner')

plotX["dummy"] = 0

cluster0 = plotX[plotX["Cluster"] == 0]
cluster1 = plotX[plotX["Cluster"] == 1]
cluster2 = plotX[plotX["Cluster"] == 2]

import plotly as py
import plotly.graph_objs as go
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot
import plotly.io as pio
pio.renderers.default = "colab"

trace1 = go.Scatter(
                    x = cluster0["PC1_1d"],
                    y = cluster0["dummy"],
                    mode = "markers",
                    name = "Cluster 0",
                    marker = dict(color = 'rgba(255, 128, 255, 0.8)'),
                    text = None)

#trace2 is for 'Cluster 1'
trace2 = go.Scatter(
                    x = cluster1["PC1_1d"],
                    y = cluster1["dummy"],
                    mode = "markers",
                    name = "Cluster 1",
                    marker = dict(color = 'rgba(255, 128, 2, 0.8)'),
                    text = None)

#trace3 is for 'Cluster 2'
trace3 = go.Scatter(
                    x = cluster2["PC1_1d"],
                    y = cluster2["dummy"],
                    mode = "markers",
                    name = "Cluster 2",
                    marker = dict(color = 'rgba(0, 255, 200, 0.8)'),
                    text = None)


data = [trace1, trace2, trace3]

title = "Visualizing Clusters in One Dimension Using PCA"

layout = dict(title = title,
              xaxis= dict(title= 'PC1',ticklen= 5,zeroline= False),
              yaxis= dict(title= '',ticklen= 5,zeroline= False)
             )

fig = dict(data = data, layout = layout)

iplot(fig)

trace1 = go.Scatter(
                    x = cluster0["PC1_2d"],
                    y = cluster0["PC2_2d"],
                    mode = "markers",
                    name = "Cluster 0",
                    marker = dict(color = 'rgba(255, 128, 255, 0.8)'),
                    text = None)

#trace2 is for 'Cluster 1'
trace2 = go.Scatter(
                    x = cluster1["PC1_2d"],
                    y = cluster1["PC2_2d"],
                    mode = "markers",
                    name = "Cluster 1",
                    marker = dict(color = 'rgba(255, 128, 2, 0.8)'),
                    text = None)

#trace3 is for 'Cluster 2'
trace3 = go.Scatter(
                    x = cluster2["PC1_2d"],
                    y = cluster2["PC2_2d"],
                    mode = "markers",
                    name = "Cluster 2",
                    marker = dict(color = 'rgba(0, 255, 200, 0.8)'),
                    text = None)


data = [trace1, trace2, trace3]

title = "Visualizing Clusters in Two Dimensions Using PCA"

layout = dict(title = title,
              xaxis= dict(title= 'PC1',ticklen= 5,zeroline= False),
              yaxis= dict(title= 'PC2',ticklen= 5,zeroline= False)
             )

fig = dict(data = data, layout = layout)

iplot(fig)

trace1 = go.Scatter3d(
                    x = cluster0["PC1_3d"],
                    y = cluster0["PC2_3d"],
                    z = cluster0["PC3_3d"],
                    mode = "markers",
                    name = "Cluster 0",
                    marker = dict(color = 'rgba(255, 128, 255, 0.8)'),
                    text = None)

#trace2 is for 'Cluster 1'
trace2 = go.Scatter3d(
                    x = cluster1["PC1_3d"],
                    y = cluster1["PC2_3d"],
                    z = cluster1["PC3_3d"],
                    mode = "markers",
                    name = "Cluster 1",
                    marker = dict(color = 'rgba(255, 128, 2, 0.8)'),
                    text = None)

#trace3 is for 'Cluster 2'
trace3 = go.Scatter3d(
                    x = cluster2["PC1_3d"],
                    y = cluster2["PC2_3d"],
                    z = cluster2["PC3_3d"],
                    mode = "markers",
                    name = "Cluster 2",
                    marker = dict(color = 'rgba(0, 255, 200, 0.8)'),
                    text = None)


data = [trace1, trace2, trace3]

title = "Visualizing Clusters in Three Dimensions Using PCA"

layout = dict(title = title,
              xaxis= dict(title= 'PC1',ticklen= 5,zeroline= False),
              yaxis= dict(title= 'PC2',ticklen= 5,zeroline= False)
             )

fig = dict(data = data, layout = layout)

iplot(fig)

